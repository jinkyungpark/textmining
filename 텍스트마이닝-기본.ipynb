{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4767e6d9",
   "metadata": {},
   "source": [
    "## 텍스트 전처리\n",
    "\n",
    "- 클렌징\n",
    "- 토큰화\n",
    "- 스톱워드\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "\n",
    "=> 전처리를 위한 NLTK 라이브러리 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a5583",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 형태로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b93ad115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SON-\n",
      "[nltk_data]     FAMILY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')   # 마침표, 개행문자 등의 데이터 세트를 다운로드\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                You can see it out your window or on your television. \\\n",
    "                You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "467a751d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Matrix is everywhere its all around us, here even in this room.',\n",
       " 'You can see it out your window or on your television.',\n",
       " 'You feel it when you go to work, or go to church or pay your taxes.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 구조로 돌아옴\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2344169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), type(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1510681",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e05692cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "\n",
    "# word_tokenize() : nltk 에서 제공하는 단어 토큰화\n",
    "words = word_tokenize(sentences)\n",
    "\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d8596",
   "metadata": {},
   "source": [
    "- 마침표나 개행문자와 같이 문장을 구분하는 구분자를 이용해 단어를 토큰화할 수 있지만, 단어의 순서가 중요하지 않은 경우 단어 토큰화만 해도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147dcfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1aac05",
   "metadata": {},
   "source": [
    "#### 여러개의 문장 데이터 -> 문장별로 단어 토큰화 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74379295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b0cac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 함수 테스트\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c322a8",
   "metadata": {},
   "source": [
    "- 여러 줄에서 먼저 문장으로 분리한 후 문장에서 다시 단어별로 분리하는 것\n",
    "- 문제점은 <b>문맥적인 의미</b>가 무시됨 => 해결하기 위해 n-gram(연속된 n 개의 단어를 하나의 토큰화) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f39e3",
   "metadata": {},
   "source": [
    "- n-gram : 문맥적인 의미를 좀 더 살리기 위한 기법(연속된 n 개의 단어를 하나의 토큰화 단위로 분리)\n",
    "- ex) \"Agent Smith knocks the door\" => (Agent, Smith),(Smith, knocks),(knocks, the),(the, door)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b020f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650fa2e1",
   "metadata": {},
   "source": [
    "### 스톱워드\n",
    "- 분석에 큰 의미가 없는 단어\n",
    "- is, the, a, will 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01034e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SON-\n",
      "[nltk_data]     FAMILY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 에서 제공하는 stopwords 다운로드\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ee18f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# stopwords가 몇 개 있고 그 중 20개만 확인\n",
    "print('영어 stop words 개수 : ',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b71b60",
   "metadata": {},
   "source": [
    "#### stopwords 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7598046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱워드를 제거하는 반복문\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    \n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱워드를 제거하는 반복문 실행\n",
    "    for word in sentence:\n",
    "        #소문자 변환\n",
    "        word = word.lower()\n",
    "        \n",
    "        # 토큰화된 개벌 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76019cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753ee0e",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d92c04",
   "metadata": {},
   "source": [
    "- 문법적인 요소에 따라 단어가 다양하게 변함(work, works, working...)\n",
    "- 원형 단어를 찾기 위한 방법\n",
    "- Stemming : 원형 단어로 변환시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    "- Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어 찾기(시간이 Stemming 보다 좀 더 걸림)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a510e44",
   "metadata": {},
   "source": [
    "**NLTK에서 제공하는 Stemmer**\n",
    "- Porter\n",
    "- Lancaster\n",
    "- Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f443425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6055e",
   "metadata": {},
   "source": [
    "- working, works, worked 중에서 work 는 기본 단어를 잘 찾아줌\n",
    "- amuse 의 경우 amuse가 원형인데 amus로 찾아냄\n",
    "- happy, fancy 의 경우에도 정확한 원형 찾기 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3a4e5",
   "metadata": {},
   "source": [
    "**NLTK에서 제공하는 Lemmatization**\n",
    "- WordNetLemmatizer 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "109dc392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\SON-\n",
      "[nltk_data]     FAMILY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#현재 제시된 단어의 품사가 무엇인지 전달 필요\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v')) # v : 동사\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))  # a : 형용사\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))  # a나 v 같은 품사를 안 주는 경우 제대로 알아맞추지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ca2d",
   "metadata": {},
   "source": [
    "## Bag of words(BOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d23ce",
   "metadata": {},
   "source": [
    "https://minerandodados.com.br/como-preparar-dados-de-texto-para-machine-learning/\n",
    "\n",
    "<img src='https://minerandodados.com.br/wp-content/uploads/2020/03/image-19-526x313.png'>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
