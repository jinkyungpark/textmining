{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 제작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용할 형태소 분석기 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "# mac\n",
    "# from konlpy.tag import Mecab\n",
    "\n",
    "# 윈도우\n",
    "import MeCab\n",
    "\n",
    "# 트위터 형태소 분석기와 이름이 같음\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "# 형태소 분석을 위한 객체 생성\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "# 윈도우에서 Mecab 생성\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "# 맥에서 Mecab 생성\n",
    "# mecab = MeCab()\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mecab_pos(text):\n",
    "    pos = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS 의 형태를 추출하는 정규 표현식\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\")\n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정\n",
    "    pos = [tuple(pattern.match(token).group(0).split(\"\\t\")) \n",
    "                   for token in mecab.parse(text).splitlines()[:-1]]\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def mecab_morphs(text):\n",
    "    morphs = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS 의 형태를 추출하는 정규 표현식\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\")\n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) \n",
    "                   for token in mecab.parse(text).splitlines()[:-1]]\n",
    "    \n",
    "    # 추출한 토큰 중에 문자열만 선택\n",
    "    for token in temp:\n",
    "        morphs.append(token[0])  \n",
    "    \n",
    "    \n",
    "    return morphs\n",
    "\n",
    "def mecab_nouns(text):\n",
    "    nouns = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS 의 형태를 추출하는 정규 표현식\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\")\n",
    "    \n",
    "    \n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) \n",
    "                   for token in mecab.parse(text).splitlines()[:-1]]\n",
    "    \n",
    "    for token in temp:\n",
    "        if token[1] == \"NNG\" or token[1]==\"NNP\" or token[1]==\"NNB\" or token[1]==\"NNBC\" or token[1]==\"NP\" or token[1]==\"NR\":\n",
    "            nouns.append(token[0]) \n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(doc):\n",
    "    \n",
    "    # 한국어를 제외한 글자를 제거하는 함수.\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\",\"\",doc)\n",
    "    \n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    \n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")    \n",
    "    \n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path,encoding='utf-8') as f:\n",
    "        for word in f:\n",
    "            SW.add(word)    \n",
    "    \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc):\n",
    "    \n",
    "    \n",
    "#     tokenized_doc = []\n",
    "    \n",
    "    # list comprehension을 풀어서 쓴 코드\n",
    "#     for word in mecab_morphs(doc):\n",
    "#         if word not in SW and len(word) > 1:\n",
    "#             tokenized_doc.append(word)\n",
    "#     return tokenized_doc\n",
    "    \n",
    "    \n",
    "    return [word for ward in mecab_morphs(doc) if word not in SW and len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = '''\n",
    "2일 서울역으로 진입하던 무궁화호 열차 1량이 탈선하는 사고가 발생했다. 이날 오후 9시 25분 용산역을 출발하기 위해 회송 중이던 열차여서 승객은 타고 있지 않았다. 코레일 측은 \"이번 사고로 인명피해는 없으나 경의선 전동열차의 운행구간을 일부 조정했다\"고 설명했다.\n",
    "한국철도공사(코레일)에 따르면 이날 오후 8시58분께 용산에서 여수엑스포역으로 출발하기 위해 회송하던 무궁화호 열차가 서울역 진입 중 맨 뒤에 달린 발전차 1량이 궤도를 이탈하는 사고가 났다. 이에 따라 이날 KTX와 일반열차는 정상운행 중이나 일부 경의선 열차 운행조정으로 지연이 발생하고 있는 것으로 알려졌다. 코레일은 사고 발생 즉시 긴급 복구반을 현장에 투입해 복구작업을 펼치고 있으며, 사고원인은 복구 이후 조사할 예정이다.\n",
    "코레일 측은 \"열차이용에 불편을 드린 점 깊이 사과드린다\"며 \"안전한 열차 운행과 복구를 위해 최선을 다하겠다\"고 밝혔다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 :  일서울역으로진입하던무궁화호열차량이탈선하는사고가발생했다이날오후시분용산역을출발하기위해회송중이던열차여서승객은타고있지않았다코레일측은이번사고로인명피해는없으나경의선전동열차의운행구간을일부조정했다고설명했다한국철도공사코레일에따르면이날오후시분께용산에서여수엑스포역으로출발하기위해회송하던무궁화호열차가서울역진입중맨뒤에달린발전차량이궤도를이탈하는사고가났다이에따라이날와일반열차는정상운행중이나일부경의선열차운행조정으로지연이발생하고있는것으로알려졌다코레일은사고발생즉시긴급복구반을현장에투입해복구작업을펼치고있으며사고원인은복구이후조사할예정이다코레일측은열차이용에불편을드린점깊이사과드린다며안전한열차운행과복구를위해최선을다하겠다고밝혔다\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "SW = define_stopwords(\"./stopwords-ko.txt\")\n",
    "\n",
    "cleaned_text = text_cleaning(text3)\n",
    "print(\"전처리 : \",cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-fd02ec16d48a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_tokenizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n형태소 분석 : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-609378d55b16>\u001b[0m in \u001b[0;36mtext_tokenizing\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mward\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmecab_morphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSW\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-609378d55b16>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mward\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmecab_morphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSW\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_text = text_tokenizing(cleaned_text)\n",
    "print(\"\\n형태소 분석 : \",tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
