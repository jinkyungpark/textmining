{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim으로 네이버 기사 토픽 모델링 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 토픽 모델링을 적용하기 위해 텍스트를 처리합니다.\n",
    "\n",
    "> 토픽 모델링 라이브러리인 gensim을 사용해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp36-cp36m-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\envs\\textmining\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda\\envs\\textmining\\lib\\site-packages (from gensim) (1.17.0)\n",
      "Requirement already satisfied: Cython==0.29.21 in d:\\anaconda\\envs\\textmining\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: smart-open, dataclasses, gensim\n",
      "Successfully installed dataclasses-0.8 gensim-4.0.1 smart-open-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링을 위한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\textmining\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Mecab, Okt 등 형태소 분석기 불러오기\n",
    "import MeCab\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 특수문자\n",
    "import string\n",
    "\n",
    "# 경고 알림 제거를 위한 라이브러리\n",
    "import warnings\n",
    "\n",
    "# gensim에서 사용하는 vectorizer 모듈과, LDA model을 불러온다.\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as pltb\n",
    "# %matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # 경고 알림이 뜨면 모두 무시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mecab_pos(text):\n",
    "    pos = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS 의 형태를 추출하는 정규 표현식\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\")\n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정\n",
    "    pos = [tuple(pattern.match(token).group(0).split(\"\\t\")) \n",
    "                   for token in mecab.parse(text).splitlines()[:-1]]\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_nouns(text):\n",
    "    nouns = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS 의 형태를 추출하는 정규 표현식\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\")\n",
    "    \n",
    "    \n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) \n",
    "                   for token in mecab.parse(text).splitlines()[:-1]]\n",
    "    \n",
    "    for token in temp:\n",
    "        if token[1] == \"NNG\" or token[1]==\"NNP\" or token[1]==\"NNB\" or token[1]==\"NNBC\" or token[1]==\"NP\" or token[1]==\"NR\":\n",
    "            nouns.append(token[0]) \n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab_morphs 도 작성\n",
    "def mecab_morphs(text):\n",
    "    morphs = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS 의 형태를 추출하는 정규 표현식\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\")\n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) \n",
    "                   for token in mecab.parse(text).splitlines()[:-1]]\n",
    "    \n",
    "    # 추출한 토큰 중에 문자열만 선택\n",
    "    for token in temp:\n",
    "        morphs.append(token[0])  \n",
    "    \n",
    "    \n",
    "    return morphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 전처리 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(input_file_name):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # pk 파일을 읽어서 리스트로 변환하여 돌려줌.\n",
    "    with open(input_file_name,'rb') as f:\n",
    "        temp_corpus = pickle.load(f)\n",
    "        \n",
    "    # 리스트안의 리스트 형태인걸 일차원 리스트로 변형    \n",
    "    for page in temp_corpus:\n",
    "        corpus += page\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 함수를 편의를 위해 조금 변형해보자.\n",
    "    cleaned_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "        cleaned_docs.append(temp_doc)\n",
    "        \n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # 특수 문자를 추가해보자. 파이썬의 특수문자 라이브러리를 이용해서 추가\n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "\n",
    "    return SW\n",
    "\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    # 명사 추출 / 형태소 분석 두 가지를 선택할 수 있게 만들어주는 함수를 만들어보자.\n",
    "     # tqdm을 사용하여 진행 과정을 볼 수 있게 만들어보자.\n",
    "    mecab = MeCab.Tagger()\n",
    "    token_corpus = []\n",
    "    \n",
    "    if tokenizer == \"noun\":  # 명사추출\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab_nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            \n",
    "            token_corpus.append(token_text)\n",
    "            \n",
    "            \n",
    "    elif tokenizer == \"morph\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab_morphs(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            \n",
    "            token_corpus.append(token_text)\n",
    "    \n",
    "    elif tokenizer == \"word\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            \n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "    return token_corpus\n",
    "\n",
    "# 함수를 불러오는 (메인) 코드.\n",
    "input_file_name = \"data/naver_news_content.pk\"\n",
    "documents = read_documents(input_file_name)\n",
    "SW = define_stopwords(\"data/stopwords-ko.txt\")\n",
    "cleaned_text = text_cleaning(documents)\n",
    "tokenized_text = text_tokenizing(cleaned_text, tokenizer=\"noun\") #tokenizer= \"noun\" or \"morph\" or \"word\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 읽기의 과정은 앞서 단어 임베딩의 경우와 다르지 않다. 다음 과정은 문서-단어 행렬을 만드는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 토픽 모델링에 사용할 함수들 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서-단어 행렬 만들기\n",
    "# 어휘(vocabulary) 학습\n",
    "\n",
    "# 문서-단어 행렬(document-term matrix) 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF 문서-단어 행렬 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA 결과 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 토픽 모델링을 추가하여 코드 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 토픽 개수, 키워드 개수를 정해주는 변수를 추가.\n",
    "\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    # 문서-단어 행렬 만들어주는 함수.\n",
    "    print(\"Building document-term matrix.\")\n",
    "\n",
    "\n",
    "\n",
    "def print_topic_words(model):\n",
    "\n",
    "    # 토픽 모델링 결과를 출력해 주는 함수.\n",
    "    print(\"\\nPrinting topic words.\\n\")\n",
    "\n",
    "\n",
    "# document-term matrix를 만들고,\n",
    "\n",
    "# LDA를 실행.\n",
    "\n",
    "# 결과를 출력.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. pyLDAvis를 통한 토픽 모델링 결과 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis 불러오기\n",
    "\n",
    "# pyLDAvis를 jupyter notebook에서 실행할 수 있게 활성화.\n",
    "\n",
    "# pyLDAvis 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트분석기법",
   "courseyear": "2018",
   "date": "2018.04.18",
   "logofile": "figs/ewhauniv-logo.png",
   "logoraise": "-.2",
   "logoscale": ".4",
   "title": "단어 임베딩과 토픽 모델링"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
